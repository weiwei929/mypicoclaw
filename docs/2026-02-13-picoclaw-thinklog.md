# 2026-02-13 PicoClaw 进化实录：从轻量级 Bot 到高可用 Agent

> **摘要**：本文记录了将开源项目 `picoclaw` (基于 `nanobot`) 改造为生产级个人 AI 助理的全过程。通过引入 128k 上下文、Gemini 容灾机制和主动会话管理，成功解决了原项目在长周期运行中的不稳定性，实现了从"玩具"到"工具"的跨越。

## 1. 缘起：寻找最轻量的 Agent 框架

在寻找个人 AI 助理方案时，我关注到了 `picoclaw`。它的核心优势在于：
- **极简架构**：基于 Go 语言，单二进制文件部署，资源占用极低（适合 1核1G VPS）。
- **扩展性**：基于 `nanobot` 核心，支持 Tool Use（工具调用）。
- **多模态潜力**：原生支持多渠道接入（Telegram 等）。

与其使用臃肿的 LangChain 或庞大的 Python 框架，不如基于这个精悍的 Golang 项目进行改造，打造一个真正属于自己的、跑在低配 VPS 上的“赛博分身”。

## 2. 挑战：生产环境下的“水土不服”

在实际部署并运行一段时间后，暴露出了几个严重影响体验的问题（详见 2026-02-12 调试记录）：

### 2.1 上下文溢出与无限循环
这是最致命的问题。原设计缺乏对长对话的有效管理：
- **现象**：当对话历史积累到一定程度（约 8k tokens），LLM 开始拒绝服务（Context Length Exceeded）。
- **后果**：Agent 陷入死循环，不断重试错误的请求，导致 CPU 飙升，日志被报错填满，最终服务崩溃。

### 2.2 脆弱的单点依赖
- **现象**：主要依赖 Kimi (Moonshot) API。一旦上游服务抖动（500/502/429），Bot 就会直接“装死”或报错。
- **痛点**：作为一个 24h 在线助理，这种稳定性是不可接受的。

### 2.3硬编码的束缚
- **发现**：在排查代码时发现，`loop.go` 中竟然硬编码了 `max_tokens: 8192` 和 `temperature: 0.7`。
- **后果**：无论在配置文件中如何修改模型参数（如想用 128k 模型），实际运行永远被锁死在 8k，导致长文本分析能力被阉割。

## 3. 进化：打造“不沉之舟”

针对上述问题，我们进行了一系列深度改造：

### 3.1 解锁长文本能力 (128k)
- **动作**：移除了代码中的硬编码限制，使其动态读取配置文件。
- **成果**：成功切换至 `moonshot-v1-128k`，上下文窗口扩大 16 倍，支持深度阅读和长对话记忆。

### 3.2 引入 Gemini 容灾机制 (High Availability)
- **架构**：设计了 Primary (Kimi) -> Fallback (Gemini) 的自动切换逻辑。
- **逻辑**：当主模型发生 5xx 错误、超时或过载时，系统自动无缝切换到 Gemini-2.0-Flash。
- **验证**：通过模拟故障测试，Bot 能在主服务挂掉的瞬间“重生”，用户甚至感觉不到切换。

### 3.3 防爆与会话管理 (Safety Valve)
为了彻底根治上下文溢出，我们实现了双重保险：
- **自动熔断 (90% Reset)**：每次请求前预检 Token 用量。一旦超过 90%（约 11w tokens），自动将当前会话归档存储，并开启新会话。
- **手动重置 (/new)**：添加了 `/new` 指令，允许用户随时通过命令“清空大脑”，开启新话题。

### 3.4 工具输出约束
- **优化**：将 `web_fetch`（网页抓取）工具的默认输出限制从 50000 字符降至 8000 字符，防止一次搜索就撑爆上下文。

## 4. 结语：开源共建

经过这一番折腾，现在的 PicoClaw 已经具备了商业级 Agent 的雏形：
- ✅ **大肚量**：128k 上下文
- ✅ **高可用**：双模容灾
- ✅ **易管理**：自动/手动归档

不仅仅是自用，这些通用性的改进（特别是配置解耦和容灾模式）也非常适合回馈给原社区。我们计划将这些成果整理为两个 PR（Feature & Bugfix）提交给上游仓库，让更多开发者受益。

从“拿来主义”到“自主魔改”，再到“反哺开源”，这正是技术折腾的乐趣所在。🦞🚀
